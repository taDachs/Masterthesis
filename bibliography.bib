@online{agarwalLeggedLocomotionChallenging2022,
  title = {Legged {{Locomotion}} in {{Challenging Terrains}} Using {{Egocentric Vision}}},
  author = {Agarwal, Ananye and Kumar, Ashish and Malik, Jitendra and Pathak, Deepak},
  date = {2022-11-14},
  eprint = {2211.07638},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.07638},
  url = {http://arxiv.org/abs/2211.07638},
  urldate = {2025-07-31},
  abstract = {Animals are capable of precise and agile locomotion using vision. Replicating this ability has been a long-standing goal in robotics. The traditional approach has been to decompose this problem into elevation mapping and foothold planning phases. The elevation mapping, however, is susceptible to failure and large noise artifacts, requires specialized hardware, and is biologically implausible. In this paper, we present the first end-to-end locomotion system capable of traversing stairs, curbs, stepping stones, and gaps. We show this result on a medium-sized quadruped robot using a single front-facing depth camera. The small size of the robot necessitates discovering specialized gait patterns not seen elsewhere. The egocentric camera requires the policy to remember past information to estimate the terrain under its hind feet. We train our policy in simulation. Training has two phases - first, we train a policy using reinforcement learning with a cheap-to-compute variant of depth image and then in phase 2 distill it into the final policy that uses depth using supervised learning. The resulting policy transfers to the real world and is able to run in real-time on the limited compute of the robot. It can traverse a large variety of terrain while being robust to perturbations like pushes, slippery surfaces, and rocky terrain. Videos are at https://vision-locomotion.github.io},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/max/Zotero/storage/DNVBDFFM/Agarwal et al. - 2022 - Legged Locomotion in Challenging Terrains using Egocentric Vision.pdf;/home/max/Zotero/storage/XLXR84X8/2211.html}
}

@online{atanassovConstrainedSkillDiscovery2024,
  title = {Constrained {{Skill Discovery}}: {{Quadruped Locomotion}} with {{Unsupervised Reinforcement Learning}}},
  shorttitle = {Constrained {{Skill Discovery}}},
  author = {Atanassov, Vassil and Yu, Wanming and Mitchell, Alexander Luis and Finean, Mark Nicholas and Havoutis, Ioannis},
  date = {2024-10-10},
  eprint = {2410.07877},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.07877},
  url = {http://arxiv.org/abs/2410.07877},
  urldate = {2024-12-22},
  abstract = {Representation learning and unsupervised skill discovery can allow robots to acquire diverse and reusable behaviors without the need for task-specific rewards. In this work, we use unsupervised reinforcement learning to learn a latent representation by maximizing the mutual information between skills and states subject to a distance constraint. Our method improves upon prior constrained skill discovery methods by replacing the latent transition maximization with a norm-matching objective. This not only results in a much a richer state space coverage compared to baseline methods, but allows the robot to learn more stable and easily controllable locomotive behaviors. We successfully deploy the learned policy on a real ANYmal quadruped robot and demonstrate that the robot can accurately reach arbitrary points of the Cartesian state space in a zero-shot manner, using only an intrinsic skill discovery and standard regularization rewards.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/max/Zotero/storage/9YXJJX5C/Atanassov et al. - 2024 - Constrained Skill Discovery Quadruped Locomotion with Unsupervised Reinforcement Learning.pdf;/home/max/Zotero/storage/Q73TF5CE/2410.html}
}

@online{atanassovCurriculumBasedReinforcementLearning2024,
  title = {Curriculum-{{Based Reinforcement Learning}} for {{Quadrupedal Jumping}}: {{A Reference-free Design}}},
  shorttitle = {Curriculum-{{Based Reinforcement Learning}} for {{Quadrupedal Jumping}}},
  author = {Atanassov, Vassil and Ding, Jiatao and Kober, Jens and Havoutis, Ioannis and Santina, Cosimo Della},
  date = {2024-03-04},
  eprint = {2401.16337},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.16337},
  url = {http://arxiv.org/abs/2401.16337},
  urldate = {2024-12-18},
  abstract = {Deep reinforcement learning (DRL) has emerged as a promising solution to mastering explosive and versatile quadrupedal jumping skills. However, current DRL-based frameworks usually rely on pre-existing reference trajectories obtained by capturing animal motions or transferring experience from existing controllers. This work aims to prove that learning dynamic jumping is possible without relying on imitating a reference trajectory by leveraging a curriculum design. Starting from a vertical in-place jump, we generalize the learned policy to forward and diagonal jumps and, finally, we learn to jump across obstacles. Conditioned on the desired landing location, orientation, and obstacle dimensions, the proposed approach yields a wide range of omnidirectional jumping motions in real-world experiments. Particularly we achieve a 90cm forward jump, exceeding all previous records for similar robots reported in the existing literature. Additionally, the robot can reliably execute continuous jumping on soft grassy grounds, which is especially remarkable as such conditions were not included in the training stage. A supplementary video can be found on: https://www.youtube.com/watch?v=nRaMCrwU5X8. The code associated with this work can be found on: https://github.com/Vassil17/Curriculum-Quadruped-Jumping-DRL.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/max/Zotero/storage/ZS3GZI3X/Atanassov et al. - 2024 - Curriculum-Based Reinforcement Learning for Quadrupedal Jumping A Reference-free Design.pdf;/home/max/Zotero/storage/VNYY9JJ5/2401.html}
}

@online{ballEfficientOnlineReinforcement2023,
  title = {Efficient {{Online Reinforcement Learning}} with {{Offline Data}}},
  author = {Ball, Philip J. and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  date = {2023-05-31},
  eprint = {2302.02948},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.02948},
  url = {http://arxiv.org/abs/2302.02948},
  urldate = {2025-09-15},
  abstract = {Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a \$\textbackslash mathbf\{2.5\textbackslash times\}\$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code at https://github.com/ikostrikov/rlpd.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/WGIPQH2M/Ball et al. - 2023 - Efficient Online Reinforcement Learning with Offline Data.pdf;/home/max/Zotero/storage/SA54IS57/2302.html}
}

@online{bellemareDistributionalPerspectiveReinforcement2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
  date = {2017-07-21},
  eprint = {1707.06887},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1707.06887},
  url = {http://arxiv.org/abs/1707.06887},
  urldate = {2025-03-30},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/max/Zotero/storage/NXPVA4NZ/Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Learning.pdf;/home/max/Zotero/storage/SM9HY4V2/1707.html}
}

@inproceedings{bledtMITCheetah32018,
  title = {{{MIT Cheetah}} 3: {{Design}} and {{Control}} of a {{Robust}}, {{Dynamic Quadruped Robot}}},
  shorttitle = {{{MIT Cheetah}} 3},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Bledt, Gerardo and Powell, Matthew J. and Katz, Benjamin and Di Carlo, Jared and Wensing, Patrick M. and Kim, Sangbae},
  date = {2018-10},
  pages = {2245--2252},
  issn = {2153-0866},
  doi = {10.1109/IROS.2018.8593885},
  url = {https://ieeexplore.ieee.org/abstract/document/8593885},
  urldate = {2025-09-15},
  abstract = {This paper introduces a new robust, dynamic quadruped, the MIT Cheetah 3. Like its predecessor, the Cheetah 3 exploits tailored mechanical design to enable simple control strategies for dynamic locomotion and features high-bandwidth proprioceptive actuators to manage physical interaction with the environment. A new leg design is presented that includes proprioceptive actuation on the abduction/adduction degrees of freedom in addition to an expanded range of motion on the hips and knees. To make full use of these new capabilities, general balance and locomotion controllers for Cheetah 3 are presented. These controllers are embedded into a modular control architecture that allows the robot to handle unexpected terrain disturbances through reactive gait modification and without the need for external sensors or prior environment knowledge. The efficiency of the robot is demonstrated by a low Cost of Transport (CoT) over multiple gaits at moderate speeds, with the lowest CoT of 0.45 found during trotting. Experiments showcase the ability to blindly climb up stairs as a result of the full system integration. These results collectively represent a promising step toward a platform capable of generalized dynamic legged locomotion.},
  eventtitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Actuators,Force,Knee,Legged locomotion,Robot sensing systems,Torque},
  file = {/home/max/Zotero/storage/ASYB3N7G/Bledt et al. - 2018 - MIT Cheetah 3 Design and Control of a Robust, Dynamic Quadruped Robot.pdf}
}

@online{brunkeSafeLearningRobotics2021,
  title = {Safe {{Learning}} in {{Robotics}}: {{From Learning-Based Control}} to {{Safe Reinforcement Learning}}},
  shorttitle = {Safe {{Learning}} in {{Robotics}}},
  author = {Brunke, Lukas and Greeff, Melissa and Hall, Adam W. and Yuan, Zhaocong and Zhou, Siqi and Panerati, Jacopo and Schoellig, Angela P.},
  date = {2021-12-06},
  eprint = {2108.06266},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2108.06266},
  url = {http://arxiv.org/abs/2108.06266},
  urldate = {2024-12-22},
  abstract = {The last half-decade has seen a steep rise in the number of contributions on safe learning methods for real-world robotic deployments from both the control and reinforcement learning communities. This article provides a concise but holistic review of the recent advances made in using machine learning to achieve safe decision making under uncertainties, with a focus on unifying the language and frameworks used in control theory and reinforcement learning research. Our review includes: learning-based control approaches that safely improve performance by learning the uncertain dynamics, reinforcement learning approaches that encourage safety or robustness, and methods that can formally certify the safety of a learned control policy. As data- and learning-based robot control methods continue to gain traction, researchers must understand when and how to best leverage them in real-world scenarios where safety is imperative, such as when operating in close proximity to humans. We highlight some of the open challenges that will drive the field of robot learning in the coming years, and emphasize the need for realistic physics-based benchmarks to facilitate fair comparisons between control and reinforcement learning approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/max/Zotero/storage/L4A8HSS5/Brunke et al. - 2021 - Safe Learning in Robotics From Learning-Based Control to Safe Reinforcement Learning.pdf;/home/max/Zotero/storage/4H73N486/2108.html}
}

@online{chane-saneCaTConstraintsTerminations2024,
  title = {{{CaT}}: {{Constraints}} as {{Terminations}} for {{Legged Locomotion Reinforcement Learning}}},
  shorttitle = {{{CaT}}},
  author = {Chane-Sane, Elliot and Leziart, Pierre-Alexandre and Flayols, Thomas and Stasse, Olivier and Souères, Philippe and Mansard, Nicolas},
  date = {2024-03-27},
  eprint = {2403.18765},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.18765},
  url = {http://arxiv.org/abs/2403.18765},
  urldate = {2025-08-11},
  abstract = {Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers to broader adoption. Through empirical evaluation on the real quadruped robot Solo crossing challenging obstacles, we demonstrate that CaT provides a compelling solution for incorporating constraints into RL frameworks. Videos and code are available at https://constraints-as-terminations.github.io.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/59B539KL/Chane-Sane et al. - 2024 - CaT Constraints as Terminations for Legged Locomotion Reinforcement Learning.pdf;/home/max/Zotero/storage/SBQMBRM2/2403.html}
}

@online{chane-saneSoloParkourConstrainedReinforcement2024,
  title = {{{SoloParkour}}: {{Constrained Reinforcement Learning}} for {{Visual Locomotion}} from {{Privileged Experience}}},
  shorttitle = {{{SoloParkour}}},
  author = {Chane-Sane, Elliot and Amigo, Joseph and Flayols, Thomas and Righetti, Ludovic and Mansard, Nicolas},
  date = {2024-09-20},
  eprint = {2409.13678},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.13678},
  url = {http://arxiv.org/abs/2409.13678},
  urldate = {2025-08-05},
  abstract = {Parkour poses a significant challenge for legged robots, requiring navigation through complex environments with agility and precision based on limited sensory inputs. In this work, we introduce a novel method for training end-to-end visual policies, from depth pixels to robot control commands, to achieve agile and safe quadruped locomotion. We formulate robot parkour as a constrained reinforcement learning (RL) problem designed to maximize the emergence of agile skills within the robot's physical limits while ensuring safety. We first train a policy without vision using privileged information about the robot's surroundings. We then generate experience from this privileged policy to warm-start a sample efficient off-policy RL algorithm from depth images. This allows the robot to adapt behaviors from this privileged experience to visual locomotion while circumventing the high computational costs of RL directly from pixels. We demonstrate the effectiveness of our method on a real Solo-12 robot, showcasing its capability to perform a variety of parkour skills such as walking, climbing, leaping, and crawling.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/87BRBG54/Chane-Sane et al. - 2024 - SoloParkour Constrained Reinforcement Learning for Visual Locomotion from Privileged Experience.pdf;/home/max/Zotero/storage/97B3GJB8/2409.html}
}

@online{chengExtremeParkourLegged2023,
  title = {Extreme {{Parkour}} with {{Legged Robots}}},
  author = {Cheng, Xuxin and Shi, Kexin and Agarwal, Ananye and Pathak, Deepak},
  date = {2023-09-25},
  eprint = {2309.14341},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.14341},
  url = {http://arxiv.org/abs/2309.14341},
  urldate = {2024-12-19},
  abstract = {Humans can perform parkour by traversing obstacles in a highly dynamic fashion requiring precise eye-muscle coordination and movement. Getting robots to do the same task requires overcoming similar challenges. Classically, this is done by independently engineering perception, actuation, and control systems to very low tolerances. This restricts them to tightly controlled settings such as a predetermined obstacle course in labs. In contrast, humans are able to learn parkour through practice without significantly changing their underlying biology. In this paper, we take a similar approach to developing robot parkour on a small low-cost robot with imprecise actuation and a single front-facing depth camera for perception which is low-frequency, jittery, and prone to artifacts. We show how a single neural net policy operating directly from a camera image, trained in simulation with large-scale RL, can overcome imprecise sensing and actuation to output highly precise control behavior end-to-end. We show our robot can perform a high jump on obstacles 2x its height, long jump across gaps 2x its length, do a handstand and run across tilted ramps, and generalize to novel obstacle courses with different physical properties. Parkour videos at https://extreme-parkour.github.io/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/max/Zotero/storage/YL5YNNFM/Cheng et al. - 2023 - Extreme Parkour with Legged Robots.pdf;/home/max/Zotero/storage/44TN4FZQ/2309.html}
}

@online{chenLearningCheating2019,
  title = {Learning by {{Cheating}}},
  author = {Chen, Dian and Zhou, Brady and Koltun, Vladlen and Krähenbühl, Philipp},
  date = {2019-12-27},
  eprint = {1912.12294},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1912.12294},
  url = {http://arxiv.org/abs/1912.12294},
  urldate = {2024-12-18},
  abstract = {Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100\% success rate on all tasks in the original CARLA benchmark, sets a new record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art. For the video that summarizes this work, see https://youtu.be/u9ZCxxD-UUw},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/JMQ8G7MH/Chen et al. - 2019 - Learning by Cheating.pdf;/home/max/Zotero/storage/9S4LXITS/1912.html}
}

@inproceedings{dicarloDynamicLocomotionMIT2018,
  title = {Dynamic {{Locomotion}} in the {{MIT Cheetah}} 3 {{Through Convex Model-Predictive Control}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Di Carlo, Jared and Wensing, Patrick M. and Katz, Benjamin and Bledt, Gerardo and Kim, Sangbae},
  date = {2018-10},
  pages = {1--9},
  issn = {2153-0866},
  doi = {10.1109/IROS.2018.8594448},
  url = {https://ieeexplore.ieee.org/document/8594448},
  urldate = {2025-09-15},
  abstract = {This paper presents an implementation of model predictive control (MPC) to determine ground reaction forces for a torque-controlled quadruped robot. The robot dynamics are simplified to formulate the problem as convex optimization while still capturing the full 3D nature of the system. With the simplified model, ground reaction force planning problems are formulated for prediction horizons of up to 0.5 seconds, and are solved to optimality in under 1 ms at a rate of 20-30 Hz. Despite using a simplified model, the robot is capable of robust locomotion at a variety of speeds. Experimental results demonstrate control of gaits including stand, trot, flying-trot, pronk, bound, pace, a 3-legged gait, and a full 3D gallop. The robot achieved forward speeds of up to 3 m/s, lateral speeds up to 1 m/s, and angular speeds up to 180 deg/sec. Our approach is general enough to perform all these behaviors with the same set of gains and weights.},
  eventtitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Convex functions,Dynamics,Legged locomotion,Predictive control,Predictive models,Robot kinematics},
  file = {/home/max/Zotero/storage/I8D9G39F/Di Carlo et al. - 2018 - Dynamic Locomotion in the MIT Cheetah 3 Through Convex Model-Predictive Control.pdf;/home/max/Zotero/storage/ZFUXPETZ/8594448.html}
}

@online{escontrelaAdversarialMotionPriors2022,
  title = {Adversarial {{Motion Priors Make Good Substitutes}} for {{Complex Reward Functions}}},
  author = {Escontrela, Alejandro and Peng, Xue Bin and Yu, Wenhao and Zhang, Tingnan and Iscen, Atil and Goldberg, Ken and Abbeel, Pieter},
  date = {2022-03-28},
  eprint = {2203.15103},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.15103},
  url = {http://arxiv.org/abs/2203.15103},
  urldate = {2024-12-18},
  abstract = {Training a high-dimensional simulated agent with an under-specified reward function often leads the agent to learn physically infeasible strategies that are ineffective when deployed in the real world. To mitigate these unnatural behaviors, reinforcement learning practitioners often utilize complex reward functions that encourage physically plausible behaviors. However, a tedious labor-intensive tuning process is often required to create hand-designed rewards which might not easily generalize across platforms and tasks. We propose substituting complex reward functions with "style rewards" learned from a dataset of motion capture demonstrations. A learned style reward can be combined with an arbitrary task reward to train policies that perform tasks using naturalistic strategies. These natural strategies can also facilitate transfer to the real world. We build upon Adversarial Motion Priors -- an approach from the computer graphics domain that encodes a style reward from a dataset of reference motions -- to demonstrate that an adversarial approach to training policies can produce behaviors that transfer to a real quadrupedal robot without requiring complex reward functions. We also demonstrate that an effective style reward can be learned from a few seconds of motion capture data gathered from a German Shepherd and leads to energy-efficient locomotion strategies with natural gait transitions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/DCF6933P/Escontrela et al. - 2022 - Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions.pdf;/home/max/Zotero/storage/QDBZQG6P/2203.html}
}

@online{fuDeepWholeBodyControl2022,
  title = {Deep {{Whole-Body Control}}: {{Learning}} a {{Unified Policy}} for {{Manipulation}} and {{Locomotion}}},
  shorttitle = {Deep {{Whole-Body Control}}},
  author = {Fu, Zipeng and Cheng, Xuxin and Pathak, Deepak},
  date = {2022-10-18},
  eprint = {2210.10044},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.10044},
  url = {http://arxiv.org/abs/2210.10044},
  urldate = {2025-08-13},
  abstract = {An attached arm can significantly increase the applicability of legged robots to several mobile manipulation tasks that are not possible for the wheeled or tracked counterparts. The standard hierarchical control pipeline for such legged manipulators is to decouple the controller into that of manipulation and locomotion. However, this is ineffective. It requires immense engineering to support coordination between the arm and legs, and error can propagate across modules causing non-smooth unnatural motions. It is also biological implausible given evidence for strong motor synergies across limbs. In this work, we propose to learn a unified policy for whole-body control of a legged manipulator using reinforcement learning. We propose Regularized Online Adaptation to bridge the Sim2Real gap for high-DoF control, and Advantage Mixing exploiting the causal dependency in the action space to overcome local minima during training the whole-body system. We also present a simple design for a low-cost legged manipulator, and find that our unified policy can demonstrate dynamic and agile behaviors across several task setups. Videos are at https://maniploco.github.io},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/max/Zotero/storage/AWDJCQUN/Fu et al. - 2022 - Deep Whole-Body Control Learning a Unified Policy for Manipulation and Locomotion.pdf;/home/max/Zotero/storage/ERDXDLBW/2210.html}
}

@online{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and family=Hoof, given=Herke, prefix=van, useprefix=false and Meger, David},
  date = {2018-10-22},
  eprint = {1802.09477},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.09477},
  url = {http://arxiv.org/abs/1802.09477},
  urldate = {2025-06-19},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/max/Zotero/storage/ZLN8JKDA/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-Critic Methods.pdf;/home/max/Zotero/storage/EV4JG4A6/1802.html}
}

@online{fuMinimizingEnergyConsumption2021,
  title = {Minimizing {{Energy Consumption Leads}} to the {{Emergence}} of {{Gaits}} in {{Legged Robots}}},
  author = {Fu, Zipeng and Kumar, Ashish and Malik, Jitendra and Pathak, Deepak},
  date = {2021-10-25},
  eprint = {2111.01674},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.01674},
  url = {http://arxiv.org/abs/2111.01674},
  urldate = {2025-05-07},
  abstract = {Legged locomotion is commonly studied and expressed as a discrete set of gait patterns, like walk, trot, gallop, which are usually treated as given and pre-programmed in legged robots for efficient locomotion at different speeds. However, fixing a set of pre-programmed gaits limits the generality of locomotion. Recent animal motor studies show that these conventional gaits are only prevalent in ideal flat terrain conditions while real-world locomotion is unstructured and more like bouts of intermittent steps. What principles could lead to both structured and unstructured patterns across mammals and how to synthesize them in robots? In this work, we take an analysis-by-synthesis approach and learn to move by minimizing mechanical energy. We demonstrate that learning to minimize energy consumption plays a key role in the emergence of natural locomotion gaits at different speeds in real quadruped robots. The emergent gaits are structured in ideal terrains and look similar to that of horses and sheep. The same approach leads to unstructured gaits in rough terrains which is consistent with the findings in animal motor control. We validate our hypothesis in both simulation and real hardware across natural terrains. Videos at https://energy-locomotion.github.io},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/VGLCBGIT/Fu et al. - 2021 - Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots.pdf;/home/max/Zotero/storage/UDDXX65N/2111.html}
}

@inproceedings{GRAMGeneralizationDeep2024,
  title = {{{GRAM}}: {{Generalization}} in {{Deep RL}} with a {{Robust Adaptation Module}}},
  shorttitle = {{{GRAM}}},
  date = {2024-10-04},
  url = {https://openreview.net/forum?id=UfczlMudN6},
  urldate = {2024-12-16},
  abstract = {The reliable deployment of deep reinforcement learning in real-world settings requires the ability to generalize across a variety of conditions, including both in-distribution scenarios seen during training as well as novel out-of-distribution scenarios. In this work, we present a framework for dynamics generalization in deep reinforcement learning that unifies these two distinct types of generalization within a single architecture. We introduce a robust adaptation module that provides a mechanism for identifying and reacting to both in-distribution and out-of-distribution environment dynamics, along with a joint training pipeline that combines the goals of in-distribution adaptation and out-of-distribution robustness. Our algorithm GRAM achieves strong generalization performance across in-distribution and out-of-distribution scenarios upon deployment, which we demonstrate on a variety of realistic simulated locomotion tasks with a quadruped robot.},
  eventtitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/max/Zotero/storage/GAJWA4Q8/2024 - GRAM Generalization in Deep RL with a Robust Adaptation Module.pdf}
}

@online{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  date = {2018-08-08},
  eprint = {1801.01290},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1801.01290},
  url = {http://arxiv.org/abs/1801.01290},
  urldate = {2025-04-16},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/max/Zotero/storage/QDEKFYXB/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf}
}

@online{hansenTDMPC2ScalableRobust2024,
  title = {{{TD-MPC2}}: {{Scalable}}, {{Robust World Models}} for {{Continuous Control}}},
  shorttitle = {{{TD-MPC2}}},
  author = {Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
  date = {2024-03-21},
  eprint = {2310.16828},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.16828},
  url = {http://arxiv.org/abs/2310.16828},
  urldate = {2025-01-27},
  abstract = {TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoderfree) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/KZGFZUE9/Hansen et al. - 2024 - TD-MPC2 Scalable, Robust World Models for Continuous Control.pdf}
}

@online{hussingDissectingDeepRL2024,
  title = {Dissecting {{Deep RL}} with {{High Update Ratios}}: {{Combatting Value Divergence}}},
  shorttitle = {Dissecting {{Deep RL}} with {{High Update Ratios}}},
  author = {Hussing, Marcel and Voelcker, Claas and Gilitschenski, Igor and Farahmand, Amir-massoud and Eaton, Eric},
  date = {2024-08-05},
  eprint = {2403.05996},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.05996},
  url = {http://arxiv.org/abs/2403.05996},
  urldate = {2025-09-18},
  abstract = {We show that deep reinforcement learning algorithms can retain their ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples by combatting value function divergence. Under large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we investigate the phenomena leading to the primacy bias. We inspect the early stages of training that were conjectured to cause the failure to learn and find that one fundamental challenge is a long-standing acquaintance: value function divergence. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be linked to overestimation on unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios, show its efficacy on the widely used dm\_control suite, and obtain strong performance on the challenging dog tasks, competitive with model-based approaches. Our results question, in parts, the prior explanation for sub-optimal learning due to overfitting early data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/E7I8LCUF/Hussing et al. - 2024 - Dissecting Deep RL with High Update Ratios Combatting Value Divergence.pdf;/home/max/Zotero/storage/IDAF69VA/2403.html}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-30},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2025-09-22},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/Y7TDXBAV/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/max/Zotero/storage/9WTFD7C8/1412.html}
}

@online{kumarRMARapidMotor2021,
  title = {{{RMA}}: {{Rapid Motor Adaptation}} for {{Legged Robots}}},
  shorttitle = {{{RMA}}},
  author = {Kumar, Ashish and Fu, Zipeng and Pathak, Deepak and Malik, Jitendra},
  date = {2021-07-08},
  eprint = {2107.04034},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2107.04034},
  url = {http://arxiv.org/abs/2107.04034},
  urldate = {2024-12-16},
  abstract = {Successful real-world deployment of legged robots would require them to adapt in real-time to unseen scenarios like changing terrains, changing payloads, wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to solve this problem of real-time online adaptation in quadruped robots. RMA consists of two components: a base policy and an adaptation module. The combination of these components enables the robot to adapt to novel situations in fractions of a second. RMA is trained completely in simulation without using any domain knowledge like reference trajectories or predefined foot trajectory generators and is deployed on the A1 robot without any fine-tuning. We train RMA on a varied terrain generator using bioenergetics-inspired rewards and deploy it on a variety of difficult terrains including rocky, slippery, deformable surfaces in environments with grass, long vegetation, concrete, pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across diverse real-world as well as simulation experiments. Video results at https://ashish-kmr.github.io/rma-legged-robots/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/VX4FA8QA/Kumar et al. - 2021 - RMA Rapid Motor Adaptation for Legged Robots.pdf;/home/max/Zotero/storage/WPE263AY/2107.html}
}

@online{leeHypersphericalNormalizationScalable2025,
  title = {Hyperspherical {{Normalization}} for {{Scalable Deep Reinforcement Learning}}},
  author = {Lee, Hojoon and Lee, Youngdo and Seno, Takuma and Kim, Donghu and Stone, Peter and Choo, Jaegul},
  date = {2025-02-21},
  eprint = {2502.15280},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.15280},
  url = {http://arxiv.org/abs/2502.15280},
  urldate = {2025-03-13},
  abstract = {Scaling up the model size and computation has brought consistent performance improvements in supervised learning. However, this lesson often fails to apply to reinforcement learning (RL) because training the model on non-stationary data easily leads to overfitting and unstable optimization. In response, we introduce SimbaV2, a novel RL architecture designed to stabilize optimization by (i) constraining the growth of weight and feature norm by hyperspherical normalization; and (ii) using a distributional value estimation with reward scaling to maintain stable gradients under varying reward magnitudes. Using the soft actor-critic as a base algorithm, SimbaV2 scales up effectively with larger models and greater compute, achieving state-of-the-art performance on 57 continuous control tasks across 4 domains. The code is available at https://dojeon-ai.github.io/SimbaV2.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/63BCWK2D/Lee et al. - 2025 - Hyperspherical Normalization for Scalable Deep Reinforcement Learning.pdf;/home/max/Zotero/storage/5475A26L/2502.html}
}

@online{leeSimBaSimplicityBias2024,
  title = {{{SimBa}}: {{Simplicity Bias}} for {{Scaling Up Parameters}} in {{Deep Reinforcement Learning}}},
  shorttitle = {{{SimBa}}},
  author = {Lee, Hojoon and Hwang, Dongyoon and Kim, Donghu and Kim, Hyunseung and Tai, Jun Jet and Subramanian, Kaushik and Wurman, Peter R. and Choo, Jaegul and Stone, Peter and Seno, Takuma},
  date = {2024-10-13},
  eprint = {2410.09754},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.09754},
  url = {http://arxiv.org/abs/2410.09754},
  urldate = {2025-03-27},
  abstract = {Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/UTLI7Z5K/Lee et al. - 2024 - SimBa Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning.pdf;/home/max/Zotero/storage/4MEAZMXK/2410.html}
}

@online{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date = {2019-07-05},
  eprint = {1509.02971},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1509.02971},
  url = {http://arxiv.org/abs/1509.02971},
  urldate = {2025-06-19},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/max/Zotero/storage/K8SHM2YU/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learning.pdf;/home/max/Zotero/storage/WC3E3CXG/1509.html}
}

@online{liRobustVersatileBipedal2023,
  title = {Robust and {{Versatile Bipedal Jumping Control}} through {{Reinforcement Learning}}},
  author = {Li, Zhongyu and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
  date = {2023-06-01},
  eprint = {2302.09450},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.09450},
  url = {http://arxiv.org/abs/2302.09450},
  urldate = {2024-12-18},
  abstract = {This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world. We present a reinforcement learning framework for training a robot to accomplish a large variety of jumping tasks, such as jumping to different locations and directions. To improve performance on these challenging tasks, we develop a new policy structure that encodes the robot's long-term input/output (I/O) history while also providing direct access to a short-term I/O history. In order to train a versatile jumping policy, we utilize a multi-stage training scheme that includes different training stages for different objectives. After multi-stage training, the policy can be directly transferred to a real bipedal Cassie robot. Training on different tasks and exploring more diverse scenarios lead to highly robust policies that can exploit the diverse set of learned maneuvers to recover from perturbations or poor landings during real-world deployment. Such robustness in the proposed policy enables Cassie to succeed in completing a variety of challenging jump tasks in the real world, such as standing long jumps, jumping onto elevated platforms, and multi-axes jumps.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/max/Zotero/storage/JAA9G4QG/Li et al. - 2023 - Robust and Versatile Bipedal Jumping Control through Reinforcement Learning.pdf;/home/max/Zotero/storage/GW4F6HGA/2302.html}
}

@online{messikommerStudentInformedTeacherTraining2025,
  title = {Student-{{Informed Teacher Training}}},
  author = {Messikommer, Nico and Xing, Jiaxu and Aljalbout, Elie and Scaramuzza, Davide},
  date = {2025-02-27},
  eprint = {2412.09149},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.09149},
  url = {http://arxiv.org/abs/2412.09149},
  urldate = {2025-08-11},
  abstract = {Imitation learning with a privileged teacher has proven effective for learning complex control behaviors from high-dimensional inputs, such as images. In this framework, a teacher is trained with privileged task information, while a student tries to predict the actions of the teacher with more limited observations, e.g., in a robot navigation task, the teacher might have access to distances to nearby obstacles, while the student only receives visual observations of the scene. However, privileged imitation learning faces a key challenge: the student might be unable to imitate the teacher's behavior due to partial observability. This problem arises because the teacher is trained without considering if the student is capable of imitating the learned behavior. To address this teacher-student asymmetry, we propose a framework for joint training of the teacher and student policies, encouraging the teacher to learn behaviors that can be imitated by the student despite the latters' limited access to information and its partial observability. Based on the performance bound in imitation learning, we add (i) the approximated action difference between teacher and student as a penalty term to the reward function of the teacher, and (ii) a supervised teacher-student alignment step. We motivate our method with a maze navigation task and demonstrate its effectiveness on complex vision-based quadrotor flight and manipulation tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/W8IAGA8Y/Messikommer et al. - 2025 - Student-Informed Teacher Training.pdf;/home/max/Zotero/storage/FA7IF3KG/2412.html}
}

@article{mnihHumanlevelControlDeep2015a,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015-02},
  journaltitle = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  url = {https://www.nature.com/articles/nature14236},
  urldate = {2025-06-19},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  langid = {english},
  keywords = {Computer science},
  file = {/home/max/Zotero/storage/WZDLLJC3/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf}
}

@online{nahrendraDreamWaQLearningRobust2023,
  title = {{{DreamWaQ}}: {{Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination}} via {{Deep Reinforcement Learning}}},
  shorttitle = {{{DreamWaQ}}},
  author = {Nahrendra, I. Made Aswin and Yu, Byeongho and Myung, Hyun},
  date = {2023-03-03},
  eprint = {2301.10602},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.10602},
  url = {http://arxiv.org/abs/2301.10602},
  urldate = {2024-12-16},
  abstract = {Quadrupedal robots resemble the physical ability of legged animals to walk through unstructured terrains. However, designing a controller for quadrupedal robots poses a significant challenge due to their functional complexity and requires adaptation to various terrains. Recently, deep reinforcement learning, inspired by how legged animals learn to walk from their experiences, has been utilized to synthesize natural quadrupedal locomotion. However, state-of-the-art methods strongly depend on a complex and reliable sensing framework. Furthermore, prior works that rely only on proprioception have shown a limited demonstration for overcoming challenging terrains, especially for a long distance. This work proposes a novel quadrupedal locomotion learning framework that allows quadrupedal robots to walk through challenging terrains, even with limited sensing modalities. The proposed framework was validated in real-world outdoor environments with varying conditions within a single run for a long distance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/max/Zotero/storage/DMTF2XWY/Nahrendra et al. - 2023 - DreamWaQ Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforc.pdf;/home/max/Zotero/storage/LJ5CPVSD/2301.html}
}

@online{nikishinPrimacyBiasDeep2022,
  title = {The {{Primacy Bias}} in {{Deep Reinforcement Learning}}},
  author = {Nikishin, Evgenii and Schwarzer, Max and D'Oro, Pierluca and Bacon, Pierre-Luc and Courville, Aaron},
  date = {2022-05-16},
  eprint = {2205.07802},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.07802},
  url = {http://arxiv.org/abs/2205.07802},
  urldate = {2025-09-19},
  abstract = {This work identifies a common flaw of deep reinforcement learning (RL) algorithms: a tendency to rely on early interactions and ignore useful evidence encountered later. Because of training on progressively growing datasets, deep RL agents incur a risk of overfitting to earlier experiences, negatively affecting the rest of the learning process. Inspired by cognitive science, we refer to this effect as the primacy bias. Through a series of experiments, we dissect the algorithmic aspects of deep RL that exacerbate this bias. We then propose a simple yet generally-applicable mechanism that tackles the primacy bias by periodically resetting a part of the agent. We apply this mechanism to algorithms in both discrete (Atari 100k) and continuous action (DeepMind Control Suite) domains, consistently improving their performance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/max/Zotero/storage/XNY27U3C/Nikishin et al. - 2022 - The Primacy Bias in Deep Reinforcement Learning.pdf;/home/max/Zotero/storage/FQHETEFA/2205.html}
}

@article{pengAMPAdversarialMotion2021,
  title = {{{AMP}}: {{Adversarial Motion Priors}} for {{Stylized Physics-Based Character Control}}},
  shorttitle = {{{AMP}}},
  author = {Peng, Xue Bin and Ma, Ze and Abbeel, Pieter and Levine, Sergey and Kanazawa, Angjoo},
  date = {2021-08-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  eprint = {2104.02180},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--20},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3450626.3459670},
  url = {http://arxiv.org/abs/2104.02180},
  urldate = {2024-12-16},
  abstract = {Synthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion tracking are a prominent class of techniques for producing high fidelity motions for a wide range of behaviors. However, the effectiveness of these tracking-based methods often hinges on carefully designed objective functions, and when applied to large and diverse motion datasets, these methods require significant additional machinery to select the appropriate motion for the character to track in a given scenario. In this work, we propose to obviate the need to manually design imitation objectives and mechanisms for motion selection by utilizing a fully automated approach based on adversarial imitation learning. High-level task objectives that the character should perform can be specified by relatively simple reward functions, while the low-level style of the character's behaviors can be specified by a dataset of unstructured motion clips, without any explicit clip selection or sequencing. These motion clips are used to train an adversarial motion prior, which specifies style-rewards for training the character through reinforcement learning (RL). The adversarial RL procedure automatically selects which motion to perform, dynamically interpolating and generalizing from the dataset. Our system produces high-quality motions that are comparable to those achieved by state-of-the-art tracking-based techniques, while also being able to easily accommodate large datasets of unstructured motion clips. Composition of disparate skills emerges automatically from the motion prior, without requiring a high-level motion planner or other task-specific annotations of the motion clips. We demonstrate the effectiveness of our framework on a diverse cast of complex simulated characters and a challenging suite of motor control tasks.},
  keywords = {Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/LUJV6QV3/Peng et al. - 2021 - AMP Adversarial Motion Priors for Stylized Physics-Based Character Control.pdf;/home/max/Zotero/storage/8VA9HQHR/2104.html}
}

@article{pengASELargeScaleReusable2022,
  title = {{{ASE}}: {{Large-Scale Reusable Adversarial Skill Embeddings}} for {{Physically Simulated Characters}}},
  shorttitle = {{{ASE}}},
  author = {Peng, Xue Bin and Guo, Yunrong and Halper, Lina and Levine, Sergey and Fidler, Sanja},
  date = {2022-07},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {41},
  number = {4},
  eprint = {2205.01906},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--17},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3528223.3530110},
  url = {http://arxiv.org/abs/2205.01906},
  urldate = {2024-12-18},
  abstract = {The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/max/Zotero/storage/2DIMWAHQ/Peng et al. - 2022 - ASE Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters.pdf;/home/max/Zotero/storage/YB6V2YE4/2205.html}
}

@online{pintoAsymmetricActorCritic2017,
  title = {Asymmetric {{Actor Critic}} for {{Image-Based Robot Learning}}},
  author = {Pinto, Lerrel and Andrychowicz, Marcin and Welinder, Peter and Zaremba, Wojciech and Abbeel, Pieter},
  date = {2017-10-18},
  eprint = {1710.06542},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1710.06542},
  url = {http://arxiv.org/abs/1710.06542},
  urldate = {2025-09-15},
  abstract = {Deep reinforcement learning (RL) has proven a powerful technique in many sequential decision making domains. However, Robotics poses many challenges for RL, most notably training on a physical system can be expensive and dangerous, which has sparked significant interest in learning control policies using a physics simulator. While several recent works have shown promising results in transferring policies trained in simulation to the real world, they often do not fully utilize the advantage of working with a simulator. In this work, we exploit the full state observability in the simulator to train better policies which take as input only partial observations (RGBD images). We do this by employing an actor-critic training algorithm in which the critic is trained on full states while the actor (or policy) gets rendered images as input. We show experimentally on a range of simulated tasks that using these asymmetric inputs significantly improves performance. Finally, we combine this method with domain randomization and show real robot experiments for several tasks like picking, pushing, and moving a block. We achieve this simulation to real world transfer without training on any real world data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/CSWG5DQN/Pinto et al. - 2017 - Asymmetric Actor Critic for Image-Based Robot Learning.pdf;/home/max/Zotero/storage/G3U8MBFH/1710.html}
}

@online{radosavovicHumanoidLocomotionNext2024,
  title = {Humanoid {{Locomotion}} as {{Next Token Prediction}}},
  author = {Radosavovic, Ilija and Zhang, Bike and Shi, Baifeng and Rajasegaran, Jathushan and Kamat, Sarthak and Darrell, Trevor and Sreenath, Koushil and Malik, Jitendra},
  date = {2024-02-29},
  eprint = {2402.19469},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.19469},
  url = {http://arxiv.org/abs/2402.19469},
  urldate = {2024-12-16},
  abstract = {We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/FXXEH2NP/Radosavovic et al. - 2024 - Humanoid Locomotion as Next Token Prediction.pdf;/home/max/Zotero/storage/NZ7RAUNP/2402.html}
}

@online{rudinParkourWildLearning2025,
  title = {Parkour in the {{Wild}}: {{Learning}} a {{General}} and {{Extensible Agile Locomotion Policy Using Multi-expert Distillation}} and {{RL Fine-tuning}}},
  shorttitle = {Parkour in the {{Wild}}},
  author = {Rudin, Nikita and He, Junzhe and Aurand, Joshua and Hutter, Marco},
  date = {2025-05-16},
  eprint = {2505.11164},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.11164},
  url = {http://arxiv.org/abs/2505.11164},
  urldate = {2025-07-31},
  abstract = {Legged robots are well-suited for navigating terrains inaccessible to wheeled robots, making them ideal for applications in search and rescue or space exploration. However, current control methods often struggle to generalize across diverse, unstructured environments. This paper introduces a novel framework for agile locomotion of legged robots by combining multi-expert distillation with reinforcement learning (RL) fine-tuning to achieve robust generalization. Initially, terrain-specific expert policies are trained to develop specialized locomotion skills. These policies are then distilled into a unified foundation policy via the DAgger algorithm. The distilled policy is subsequently fine-tuned using RL on a broader terrain set, including real-world 3D scans. The framework allows further adaptation to new terrains through repeated fine-tuning. The proposed policy leverages depth images as exteroceptive inputs, enabling robust navigation across diverse, unstructured terrains. Experimental results demonstrate significant performance improvements over existing methods in synthesizing multi-terrain skills into a single controller. Deployment on the ANYmal D robot validates the policy's ability to navigate complex environments with agility and robustness, setting a new benchmark for legged robot locomotion.},
  pubstate = {prepublished},
  keywords = {Computer Science - Robotics},
  file = {/home/max/Zotero/storage/RD7C7AG9/Rudin et al. - 2025 - Parkour in the Wild Learning a General and Extensible Agile Locomotion Policy Using Multi-expert Di.pdf;/home/max/Zotero/storage/47TVZJAJ/2505.html}
}

@online{schneiderLearningRiskAwareQuadrupedal2024,
  title = {Learning {{Risk-Aware Quadrupedal Locomotion}} Using {{Distributional Reinforcement Learning}}},
  author = {Schneider, Lukas and Frey, Jonas and Miki, Takahiro and Hutter, Marco},
  date = {2024-05-03},
  eprint = {2309.14246},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.14246},
  url = {http://arxiv.org/abs/2309.14246},
  urldate = {2024-12-19},
  abstract = {Deployment in hazardous environments requires robots to understand the risks associated with their actions and movements to prevent accidents. Despite its importance, these risks are not explicitly modeled by currently deployed locomotion controllers for legged robots. In this work, we propose a risk sensitive locomotion training method employing distributional reinforcement learning to consider safety explicitly. Instead of relying on a value expectation, we estimate the complete value distribution to account for uncertainty in the robot's interaction with the environment. The value distribution is consumed by a risk metric to extract risk sensitive value estimates. These are integrated into Proximal Policy Optimization (PPO) to derive our method, Distributional Proximal Policy Optimization (DPPO). The risk preference, ranging from risk-averse to risk-seeking, can be controlled by a single parameter, which enables to adjust the robot's behavior dynamically. Importantly, our approach removes the need for additional reward function tuning to achieve risk sensitivity. We show emergent risk sensitive locomotion behavior in simulation and on the quadrupedal robot ANYmal. Videos of the experiments and code are available at https://sites.google.com/leggedrobotics.com/risk-aware-locomotion.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/UVK4NYCG/Schneider et al. - 2024 - Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning.pdf;/home/max/Zotero/storage/7NUYA2M5/2309.html}
}

@online{schulmanProximalPolicyOptimization2017b,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-07-20},
  url = {https://arxiv.org/abs/1707.06347v2},
  urldate = {2025-06-19},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/max/Zotero/storage/9P9694P7/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf}
}

@online{seoFastTD3SimpleFast2025,
  title = {{{FastTD3}}: {{Simple}}, {{Fast}}, and {{Capable Reinforcement Learning}} for {{Humanoid Control}}},
  shorttitle = {{{FastTD3}}},
  author = {Seo, Younggyo and Sferrazza, Carmelo and Geng, Haoran and Nauman, Michal and Yin, Zhao-Heng and Abbeel, Pieter},
  date = {2025-05-28},
  url = {https://arxiv.org/abs/2505.22642v3},
  urldate = {2025-06-19},
  abstract = {Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/max/Zotero/storage/HWGCQTIF/Seo et al. - 2025 - FastTD3 Simple, Fast, and Capable Reinforcement Learning for Humanoid Control.pdf}
}

@article{silverDeterministicPolicyGradient,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  langid = {english},
  file = {/home/max/Zotero/storage/HMYX3L8M/Silver et al. - Deterministic Policy Gradient Algorithms.pdf}
}

@online{songObservationalOverfittingReinforcement2019,
  title = {Observational {{Overfitting}} in {{Reinforcement Learning}}},
  author = {Song, Xingyou and Jiang, Yiding and Tu, Stephen and Du, Yilun and Neyshabur, Behnam},
  date = {2019-12-28},
  eprint = {1912.02975},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1912.02975},
  url = {http://arxiv.org/abs/1912.02975},
  urldate = {2025-09-19},
  abstract = {A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL).},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/max/Zotero/storage/LPZQ9PVD/Song et al. - 2019 - Observational Overfitting in Reinforcement Learning.pdf;/home/max/Zotero/storage/IJNP9RT3/1912.html}
}

@book{suttonReinforcementLearningIntroduction1998,
  title = {Reinforcement Learning: {{An}} Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {1998},
  volume = {1},
  number = {1},
  publisher = {MIT press Cambridge},
  url = {https://www.cambridge.org/core/journals/robotica/article/robot-learning-edited-by-jonathan-h-connell-and-sridhar-mahadevan-kluwer-boston-19931997-xii240-pp-isbn-0792393651-hardback-21800-guilders-12000-8995/737FD21CA908246DF17779E9C20B6DF6},
  urldate = {2025-06-17},
  file = {/home/max/Zotero/storage/5N9MBJD9/Sutton and Barto - 1998 - Reinforcement learning An introduction.pdf;/home/max/Zotero/storage/73MJ2RXC/Sutton and Barto - 1998 - Reinforcement learning An introduction.pdf}
}

@online{vanhasseltDeepReinforcementLearning2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q-learning}}},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=true and Guez, Arthur and Silver, David},
  date = {2015-09-22},
  url = {https://arxiv.org/abs/1509.06461v3},
  urldate = {2025-06-19},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  langid = {english},
  organization = {arXiv.org},
  file = {/home/max/Zotero/storage/TXX8PSH5/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf}
}

@online{wuDayDreamerWorldModels2022,
  title = {{{DayDreamer}}: {{World Models}} for {{Physical Robot Learning}}},
  shorttitle = {{{DayDreamer}}},
  author = {Wu, Philipp and Escontrela, Alejandro and Hafner, Danijar and Goldberg, Ken and Abbeel, Pieter},
  date = {2022-06-28},
  eprint = {2206.14176},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.14176},
  url = {http://arxiv.org/abs/2206.14176},
  urldate = {2025-01-27},
  abstract = {To solve tasks in complex environments, robots need to learn from experience. Deep reinforcement learning is a common approach to robot learning but requires a large amount of trial and error to learn, limiting its deployment in the physical world. As a consequence, many advances in robot learning rely on simulators. On the other hand, learning inside of simulators fails to capture the complexity of the real world, is prone to simulator inaccuracies, and the resulting behaviors do not adapt to changes in the world. The Dreamer algorithm has recently shown great promise for learning from small amounts of interaction by planning within a learned world model, outperforming pure reinforcement learning in video games. Learning a world model to predict the outcomes of potential actions enables planning in imagination, reducing the amount of trial and error needed in the real environment. However, it is unknown whether Dreamer can facilitate faster learning on physical robots. In this paper, we apply Dreamer to 4 robots to learn online and directly in the real world, without simulators. Dreamer trains a quadruped robot to roll off its back, stand up, and walk from scratch and without resets in only 1 hour. We then push the robot and find that Dreamer adapts within 10 minutes to withstand perturbations or quickly roll over and stand back up. On two different robotic arms, Dreamer learns to pick and place multiple objects directly from camera images and sparse rewards, approaching human performance. On a wheeled robot, Dreamer learns to navigate to a goal position purely from camera images, automatically resolving ambiguity about the robot orientation. Using the same hyperparameters across all experiments, we find that Dreamer is capable of online learning in the real world, establishing a strong baseline. We release our infrastructure for future applications of world models to robot learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/max/Zotero/storage/ZFLA6TC3/Wu et al. - 2022 - DayDreamer World Models for Physical Robot Learning.pdf;/home/max/Zotero/storage/7QNVIWHB/2206.html}
}

@inproceedings{zangerSafeContinuousControl2021,
  title = {Safe Continuous Control with Constrained Model-Based Policy Optimization},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Zanger, Moritz A. and Daaboul, Karam and Zöllner, J. Marius},
  date = {2021},
  pages = {3512--3519},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/9635984/},
  urldate = {2025-01-13},
  file = {/home/max/Zotero/storage/AKDVYXFG/Zanger et al. - 2021 - Safe continuous control with constrained model-based policy optimization.pdf}
}
