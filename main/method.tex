% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Method}\label{chap:method}

The implementation builds on FastTD3~\citep{seoFastTD3SimpleFast2025} and is extended with a
fixed-proportion offline buffer, a recurrent policy for temporal fusion, and an egocentric-vision
pathway on the actor side. Unless stated otherwise, choices follow the background in
\autoref{chap:foundations}.

All concrete hyperparameters (batch sizes, sequence lengths, $n$-step targets, number of critics
when ensembling, support size for the distributional head, and optimizer settings) are reported in
the table accompanying this chapter.

\section{Environment and Observations}\label{sec:environment}

Experiments use IsaacLab with a Unitree GO2 robot across the following scenes:

\begin{itemize}
  % \item \textbf{Rough}: uneven terrain generated as heightfield noise.
  \item \textbf{Gap}: start on a platform with a discontinuity to the main floor; requires precise
    foot placement to step across.
  \item \textbf{Ring}: a suspended ring constrains the workspace; progress requires crouching and
    ducking under.
  \item \textbf{Box}: stacked boxes form descending steps; the robot must down-climb.
  \item \textbf{Pit}: a two-tier pit; recovery requires climbing up successive ledges.
  \item \textbf{Pyramid Stairs}: ascending stairs with increasing tread/riser variation.
  \item \textbf{Inverse Pyramid Stairs}: descending counterpart to the above.
\end{itemize}

\begin{figure}
  \centering
    \includegraphics[width=0.95\textwidth]{figures/sim_tasks.png}
  \caption{Images taken in the simulation environment used for training. The obstacles get
  progressively harder, e.g. the gap that the robot has to jump across increases or the height the
  robot has to crawl below gets smaller.}
  \label{fig:sim-tasks}
\end{figure}


The student observations comprise base angular velocity, projected gravity, velocity commands,
relative joint positions and velocities, previous action, and a $1\times48\times48$ depth image.
The critic and teacher receive the student channels \emph{plus} privileged inputs (base
velocity and height/roof scans); vision is excluded from both teacher and critic.  Commands are
sampled as target headings around the robot and converted to velocity commands.  Domain
randomization is applied over friction, mass, and external pushes; occasional pushes are injected
during rollouts.

\section{Action Outputs, Scaling, and PD Control}\label{sec:action-scaling}

Actions command joint \emph{position targets} relative to a default pose via
\[
q \;=\; a \cdot \omega \;+\; \hat{q},
\]
where $a$ are the policy outputs, $\omega$ is a scalar, and $\hat{q}$ is the
default joint configuration.  Targets are tracked with PD control; torques are proportional to the
position error. Two practical points follow:

\begin{itemize}
  \item \textbf{Effective work space.} Reducing $\omega$ shrinks the angular work space (blue
    sectors in \autoref{fig:action-space}). Near the edges of a small work space, the policy has
    little room to generate additional error, hence less PD torque. This makes stepping over
    obstacles and fast recoveries harder.
  \item \textbf{Histogram behavior.} With too small $\omega$, policies tend to sit near the
    reachable boundary, producing spiky action histograms; with a larger $\omega$, the distribution
    spreads and the controller has headroom to create or dissipate momentum.
    \autoref{fig:action-space} illustrates these regimes for a representative joint (motor~8).
\end{itemize}

In all experiments, actions are clipped to the full actuator bounds of the simulator. Squashing is
not required for stability under this setup.

\begin{figure}[h]
  \centering
\begin{subfigure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/joint_range_action_scaling.png}
  \end{subfigure}
  \begin{subfigure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/joint-range-histogram.png}
  \end{subfigure}
  \caption{Effect of the action-range parameter $\omega$ on reachable joint work space (left) and on
  policy action histograms for a representative joint (right). Larger $\omega$ allows additional
error to be commanded away from the default pose $\hat{q}$, which increases available PD torque;
small $\omega$ constrains motion and leads to boundary concentration.}
  \label{fig:action-space}
\end{figure}


\section{Architecture}\label{sec:architecture}

Two architecture families are evaluated for \emph{both} policy and critic:
\begin{itemize}
  \item \textbf{MLP}: three feed-forward layers (ReLU). LayerNorm can be enabled.
  \item \textbf{Simba-style}: as described in \cite{leeHypersphericalNormalizationScalable2025}
\end{itemize}
The critic is distributional with fixed support and is used either as clipped double or as an
ensemble.  A recurrent variant places a GRU in front of the torso; fixed-length sequences are
sampled from replay with masking after the first terminal and optional burn-in (no gradients through
burn-in).

Exploration noise is scheduled independently per parallel environment and
resampled on episode termination.

\subsection{Teacher-Student Setup}\label{sec:teacher-student-setup}

Two approaches for introducing a teacher were evaluated. The first one uses an additional offline
buffer filled with data collected from a teacher policy with privileged observations ; the same
privileged channels are provided to the critic during student training (no vision). Mini-batches are
sampled as a constant mixture: half offline (teacher) and half online (student). The mixture is not
annealed.

The second approach utilizes the teacher policy \( \pi_{\text{teacher}} \) by adding an additional
behavior cloning term to the loss function of the policy:

\[
  \pi = \text{arg max}_\pi \ \mathbb{E}_{s \sim \rho_\pi} [ Q(s, \pi(s)) - \lambda(t) \cdot || \pi(s) -
  \pi_{\text{teacher}}(s)||^2],
\]

where \( \lambda(t) = \lambda_{\text{max}} \cdot e ^ {- \alpha t / T} \) is annealed over time with \( t
\) being the current timestep, \( T \) the maximal timestep and \( \alpha \) a hyperparameter. The
basid idea is to let the teacher guide the beginning of the training with the off-policy RL taking
over later during training.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.95
    \textwidth]{figures/method-workflow.png}
    \end{center}
    \caption{The different approaches visualized. As a first step, the teacher policy is trained using
    privileged information. For the offline-buffer approach, a replay buffer separate from the
    online-buffer is filled using data collected by the teacher policy. This allows to bootstrap the
    training using expert data. In the hybrid approach, the teacher is used to directly supervise the
    student policy via knowledge distillation in addition to the normal off-policy training.}
  \label{fig:method-worflow}
\end{figure}

\subsection{Vision (Actor-Only)}\label{sec:vision}

Egocentric depth ($1\times48\times48$) is encoded by a compact CNN (conv–LeakyReLU–maxpool stacks)
into a latent vector that is concatenated to the policy input; the critic does not receive images
and instead uses privileged geometric channels (height/roof scans).  Training-time augmentation uses
random shift, low-amplitude Gaussian noise, and cutout.  An auxiliary decoder reconstructs a height
map from the latent; this loss is used only as a regularizer and is not used at test time.


\section{Training}

The training took place in IsaacSim, a highly parallel simulation environment. While the PPO teacher
could be trained with 4096 environments, the student policies could only use 512 environments due to
the increase memory footprint of the generated depth images.

\subsection{Batch Construction and Update-to-Data}

Large batches are formed by sampling uniformly from replay.  For feed-forward updates, the effective
batch size equals the number of sampled items across all environments.  For recurrent updates, the
sampler returns fixed-length sequences per environment; losses are masked after the first terminal
within each sequence.  The concrete batch sizes, sequence lengths, number of updates per environment
step, and policy-update delay are summarized in the hyperparameter table.

\section{Deployment}
\label{sec:deployment}

To test the sim2real the policy is evaluated on a real robot. The robot is equipped with a payload
containing an Intel NUC, as well as a Realsense depth camera. To make real-time inference possible
ONNX is used as the inference engine. As the internal computer of the robot is closed off, DDS is
used to receive measurements of the internal sensors and for sending position commands to the
motors. The parameters of the internal PD controllers are set to the same values used in the
simulation.

The depth images are preprocessed by sampling them down to the resolution used during training
(48x48), and thresholded to a maximum distance of 1m.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.6
    \textwidth]{figures/deployment.png}
    \end{center}
    \caption{The trained student policy is deployed on the robot using a payload with an Intel NUC.
    The NUC communicates with the robots onboard-computer using DDS, while also reading depth images
    from the realsense and preprocessing the data.}
  \label{fig:deployment}
\end{figure}
