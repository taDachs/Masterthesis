\chapter{Related Work}

This chapter reviews prior work relevant to scaling off-policy reinforcement learning and to
locomotion policies that operate from high-level observations such as depth images or point clouds.
The first part discusses challenges and recent approaches for scaling off-policy methods, with a
particular focus on stability issues, normalization strategies, and data-throughput techniques. The
second part surveys perception-based locomotion, covering geometric and egocentric representations,
temporal fusion, visual regularization, and the use of expert data through imitation or
demonstration-augmented off-policy learning. Together, these lines of work provide the context for
the methods and design choices explored in this thesis.

\section{Scaling Off-Policy RL}\label{sec:scaling-off-policy-rl}

\cite{naumanBiggerRegularizedOptimistic2024} identify two central axes of scaling in off-policy RL:
model architecture and the update-to-data (UTD) ratio. In principle, larger models should allow
more expressive function approximation, but unlike in supervised learning, increasing network
capacity in RL often \emph{reduces} performance \citep{bjorckDeeperDeepReinforcement2022}.

One difficulty arises from bootstrapping. Higher UTD improves sample reuse but also amplifies
instabilities in the value function. Recent analyses argue that the dominant failure mode is not
simply overfitting early data but \emph{value divergence}: overly optimistic targets combine with
optimizer momentum to establish a primacy effect that reduces later plasticity
\citep{hussingDissectingDeepRL2024, nikishinPrimacyBiasDeep2022}. Large early value updates inflate
$Q$ for both in-distribution and out-of-distribution (OOD) actions, and repeated bootstrapping at
high UTD locks in these errors before the replay distribution widens. Periodic partial
re-initialization can restore plasticity but increases computational cost
\citep{doroSampleEfficientReinforcementLearning2022}.

A related instability is \emph{extrapolation error}: actor updates query the critic on regions of
the action space insufficiently covered by the buffer. Function approximation extrapolates, and
the target/argmax operator preferentially selects optimistic estimates. Proposed mitigations
include distributional critics with fixed support
\citep{bellemareDistributionalPerspectiveReinforcement2017} and critic ensembles that reduce
target variance at higher UTD \citep{chenRandomizedEnsembledDouble2021,
leeHypersphericalNormalizationScalable2025}. Without additional stabilization, merely deepening or
widening networks tends to worsen these issues \citep{bjorckDeeperDeepReinforcement2022}.

One line of work focuses on improving critic stability through \emph{distributional} value
estimation. Instead of regressing to a single scalar $Q(s,a)$, distributional critics approximate
the full return distribution $Z(s,a)$ and train using a distributional variant of the Bellman
operator. The C51 algorithm \citep{bellemareDistributionalPerspectiveReinforcement2017} models
$Z(s,a)$ as a categorical distribution on a fixed support and applies a projected Bellman update
that preserves this support. By learning a richer return representation, C51 reduces target variance
and provides a more informative training signal for the critic. Distributional value functions have
been shown to improve performance across a variety of deep RL benchmarks, and their fixed-support
formulation interacts favorably with bootstrapping, preventing the drift in value magnitude often
observed under high update-to-data settings.

Two recent approaches offer complementary strategies for scaling. SimbaV2
\citep{leeHypersphericalNormalizationScalable2025} stabilizes training through a combination of
hyperspherical normalization and distributional value estimation. The method constrains both
weights and intermediate features to lie on a unit hypersphere, decoupling parameter direction
from magnitude and ensuring that gradient updates do not lead to uncontrolled growth in activation
norms. This improves the conditioning of both actor and critic updates and reduces sensitivity to
optimizer momentum, which is particularly important when scaling network width or depth. Paired
with a distributional critic and reward scaling, SimbaV2 produces more stable value targets and
reduces variance in TD updates, enabling deeper networks and allowing somewhat higher UTD settings
without the rapid divergence observed in standard architectures. The main drawback is the
additional computational overhead of the normalization and projection operations, but the authors
report consistent gains across a range of continuous-control benchmarks, suggesting that careful
architectural normalization can mitigate many of the scaling pathologies seen in off-policy RL.

FastTD3 \citep{seoFastTD3SimpleFast2025} takes a different approach by focusing on data throughput
rather than architectural modifications. The method uses large-scale parallel simulation to collect
substantially more \emph{fresh} experience per unit time, reducing the reliance on repeated reuse
of the replay buffer and thereby lowering the effective UTD. This allows training to proceed with
a standard MLP architecture while avoiding many of the instabilities associated with aggressive
bootstrapping. FastTD3 performs updates using very large batch sizes , reportedly up to 32k , leading
to lower gradient variance and a more stable learning signal. The algorithm retains the structure
of TD3 \citep{fujimotoAddressingFunctionApproximation2018}, including delayed policy updates and
scheduled exploration noise, but augments it with a distributional critic similar to
\cite{bellemareDistributionalPerspectiveReinforcement2017} to further improve the stability of
value estimation under large batches. In practice, this combination yields substantially faster
wall-clock learning and improved robustness across tasks. Notably, SimbaV2 and FastTD3 are
compatible, as demonstrated in \citep{seoFastTD3SimpleFast2025}, where the normalization techniques
of SimbaV2 are applied on top of the high-throughput training recipe of FastTD3.


\section{Locomotion Using High-Level Observations}\label{sec:locomotion-high-level-obs}

Reliable locomotion in structured indoor settings or unstructured outdoor terrain typically
requires \emph{exteroceptive} sensing. Modern legged platforms use depth cameras or LiDAR,
leading to a choice between explicit geometric or semantic representations and end-to-end
egocentric perception.

Point-cloud–based approaches provide direct geometric information but are affected by noise,
occlusions, and self-occlusions. \cite{hoellerNeuralSceneRepresentation2022} propose a
reconstruction network to complete the environment from partial sensor data; policies can be
trained directly on the reconstructed representation
\citep{rudinAdvancedSkillsLearning2022, hoellerANYmalParkourLearning2023}. Reconstruction quality
degrades in highly unstructured terrain and depends on low-latency, accurate state estimation.
During agile motions, estimator delay and transient errors can reduce reconstruction fidelity and
downstream control performance \citep{rudinParkourWildLearning2025}.

An alternative line of work relies on egocentric vision. Here, expert policies with privileged
state (e.g., elevation maps) are trained using on-policy RL, and their behavior is transferred to a
student policy that receives only camera observations, often via DAgger
\citep{rossReductionImitationLearning2011}. Students commonly incorporate GRU or LSTM modules to
counteract partial observability. Empirically, such students can show reduced zero-shot
generalization on unseen terrain due to the modality shift. \cite{rudinParkourWildLearning2025}
report that a short simulation fine-tuning phase helps the student adapt to its sensing modality,
including behaviors such as camera tilting that make observations more informative.

Temporal fusion plays an important role in egocentric policies. High-level observations are
partial and view-dependent; short histories help recover latent properties such as terrain slope,
obstacle layout, or motion-induced depth distortions. Approaches range from frame stacking to
lightweight recurrent modules. For off-policy RL, temporal fusion interacts with critic stability
and UTD, as repeatedly backing up correlated histories can increase gradient variance. Modest
history lengths, gated recurrence with normalization, and distributional or ensembled critics are
commonly used to address these issues.

Visual augmentation and normalization techniques are also critical. Random shifts, crops, and
photometric perturbations regularize the encoder and reduce overfitting to early replay contents,
enabling low UTD (1–2) without sacrificing sample efficiency. Per-channel input normalization and
feature normalization (e.g., LayerNorm or norm-decoupled layers) further stabilize the critic by
controlling activation scales, reducing target-gradient variance and mitigating OOD optimism
\citep{kostrikovImageAugmentationAll2021}.

Beyond encoder-level augmentation, domain randomization is widely used to improve the robustness
and transferability of vision-based locomotion policies. Randomizing lighting, textures, camera
parameters, and scene geometry prevents the policy from overfitting to specific simulation
conditions and encourages feature extraction that is invariant to nuisance factors in the visual
input. Recent work on legged locomotion demonstrates that extensive randomization of visual and
physical properties is essential for successful Sim2Real transfer when training with high-level
observations \citep{rudinAdvancedSkillsLearning2022, rudinParkourWildLearning2025}. These
procedures typically include randomized terrain appearance, camera noise, motion blur, and dynamic
perturbations of both robot and environment. When combined with strong augmentation and temporal
fusion, domain randomization helps mitigate the sensitivity of egocentric policies to lighting
changes and sensor artifacts, making them more robust to deployment in real-world conditions.

\section{Expert Data}\label{sec:expert-data}

Finally, expert data can be incorporated either through supervised imitation or through
demonstration-augmented off-policy RL. In the imitation (supervised) paradigm, one first trains
an expert policy under privileged state representation (e.g., full terrain information or elevation
maps). The expert executes a set of trajectories solving the task robustly. A student policy with
egocentric observations (e.g., onboard vision or depth cameras) then learns to replicate those
trajectories via behavior cloning or iterative dataset aggregation (DAgger)
\citep{rossReductionImitationLearning2011}. To compensate for partial observability and perceptual
aliasing, the student often employs temporal-fusion networks (e.g., GRU or LSTM) to reconstruct
latent state variables such as terrain slope or obstacle layout. In several works, after cloning,
authors further fine-tune the student in simulation under its own observation modality; this
fine-tuning allows the student to adjust behavior (e.g., foot placement, body posture, camera tilt)
to better suit the limited sensing \citep{rudinParkourWildLearning2025,
hoellerANYmalParkourLearning2023}. This two-stage workflow  ,  expert training + imitation +
modality-aware fine-tuning  ,  is a common template when transferring privileged-state skills to
perception-based controllers.

In contrast, demonstration-augmented off-policy RL integrates expert trajectories directly into the
learning process, rather than relying solely on supervised cloning. For example, RLPD (Reinforcement
Learning with Prior Data) \citep{ballEfficientOnlineReinforcement2023} shows that a standard
off-policy algorithm can leverage a fixed offline demonstration buffer in parallel with online
experience to improve sample efficiency and stability. RLPD augments replay with expert (or prior)
transitions and applies only minimal modifications, careful data sampling, normalization of critic
updates, and use of ensembles, yet obtains strong performance gains across diverse benchmarks.

In this setup, expert transitions influence critic updates from the very beginning, anchoring value
estimates with trajectories known to achieve high returns; the policy need not match expert actions
exactly, but is encouraged to rediscover or refine good behaviors under its own observation model.
Demonstration-augmented methods thus reduce reliance on pure exploration, especially early in
training when the replay buffer is sparse or uninformative. As a result, these methods can achieve
higher sample efficiency and faster convergence compared to both pure off-policy RL and naive
cloning, while preserving the ability to improve beyond the behavior of the expert data.

However, most existing work focuses on demonstration reuse under the same or similar observation
modalities, and rarely addresses how to adapt expert trajectories collected under privileged sensing
(e.g., elevation maps or perfect geometry) to students receiving only egocentric, partial
observations. This modality shift introduces additional challenges  ,  the agent must re-interpret the
expert trajectories under a degraded sensor model, which may require re-learning perceptual
affordances or adjusting behavior. As far as the literature shows, these questions remain largely
open.

